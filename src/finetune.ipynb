{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "finetune.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXQVrIQ3Nreq"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gGq95942DKk",
        "outputId": "18336e16-dbfd-4acf-bcea-6849488bacf7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/CSC420/CSC420_project-main/src'\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/CSC420/CSC420_project-main/src\n",
            " arch\t\t\t   infer.py\t results_1\t train_out\n",
            " arch_st\t\t   __init__.py\t results_1e-06\t train.py\n",
            "'Copy of finetune.ipynb'   log.log\t results_1e-07\t util\n",
            " data\t\t\t   main.py\t results_1e-09\t util.ipynb\n",
            " finetune.ipynb\t\t   pretrained\t test_out\t val_out\n",
            " generate_lr.py\t\t   results_0.1\t test.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDE81ZWf1q5Q"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "logging.basicConfig(filename='./log.log')\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from arch.srgan_model import Generator, Discriminator\n",
        "from arch.vgg19 import vgg19\n",
        "from arch.losses import TVLoss, perceptual_loss\n",
        "from util import arg_util\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import random\n",
        "import multiprocessing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YK3rX3XNvp7"
      },
      "source": [
        "### Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G9BSC-IAbjc"
      },
      "source": [
        "aug = transforms.Compose([\n",
        "    transforms.RandomAffine(\n",
        "        degrees=180, \n",
        "        translate=(0.2, 0.2), \n",
        "        scale=(0.7, 1.3),\n",
        "        shear=40,\n",
        "        resample=Image.BICUBIC, \n",
        "        fillcolor=255\n",
        "    ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    # torchvision.transforms.ColorJitter(\n",
        "    #     brightness=0.01, \n",
        "    #     contrast=0.2, \n",
        "    #     saturation=0.1, \n",
        "    #     hue=0.01\n",
        "    # )\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gACJO_oI7DQz"
      },
      "source": [
        "IMG_EXTENSIONS = set(['.jpg', '.jpeg', '.png', '.ppm', '.bmp', 'tiff'])\n",
        "def is_image(path):\n",
        "    return path.suffix.lower() in IMG_EXTENSIONS\n",
        "\n",
        "class LowResGroundTruthDataset(Dataset):\n",
        "    \"\"\"Training Dataset for use when training an SR model.\"\"\"\n",
        "    def __init__(self, lr_dir, gt_dir, memcache=False, transform=None,\n",
        "                 strict_filename_intersection=True):\n",
        "        super().__init__()\n",
        "        self._DataLoader__initialized = False\n",
        "        self.lr_dir = pathlib.Path(lr_dir)\n",
        "        self.gt_dir = pathlib.Path(gt_dir)\n",
        "        self.memcache = memcache\n",
        "        self.transform = transform\n",
        "\n",
        "        # Attempt filename matching.\n",
        "        self.lr_image_filepaths = [f for f in self.lr_dir.glob('*') if is_image(f)]\n",
        "        self.gt_image_filepaths = [f for f in self.gt_dir.glob('*') if is_image(f)]\n",
        "\n",
        "        lr_image_filenames = set(map(os.path.basename, self.lr_image_filepaths))\n",
        "        gt_image_filenames = set(map(os.path.basename, self.gt_image_filepaths))\n",
        "        intersect_filenames = lr_image_filenames.intersection(gt_image_filenames)\n",
        "        if strict_filename_intersection:\n",
        "            mismatched_filenames = (lr_image_filenames.union(gt_image_filenames)).difference(intersect_filenames)\n",
        "            if len(mismatched_filenames) > 0:\n",
        "                raise ValueError(f\"Mismatched filenames in lr_dir and gt_dir: {str(mismatched_filenames)}\")\n",
        "\n",
        "        self.image_filenames = list(sorted(intersect_filenames))\n",
        "        self.image_lr_gt_pairs = []\n",
        "\n",
        "        # Load the images if we want to cache them in memory.\n",
        "        if self.memcache:\n",
        "            for i, img_filename in enumerate(self.image_filenames):\n",
        "                # Images with Shape: (C, H, W)\n",
        "                img_lr = Image.open(os.path.join(self.lr_dir, img_filename)).convert(\"RGB\")\n",
        "                img_gt = Image.open(os.path.join(self.gt_dir, img_filename)).convert(\"RGB\")\n",
        "                self.image_lr_gt_pairs.append((img_lr, img_gt))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img_filename = self.image_filenames[i]\n",
        "        if self.memcache:\n",
        "            img_lr, img_gt = self.image_lr_gt_pairs[i]\n",
        "        else:\n",
        "            # Images with Shape: (C, H, W)\n",
        "            img_lr = Image.open(os.path.join(self.lr_dir, img_filename)).convert(\"RGB\")\n",
        "            img_gt = Image.open(os.path.join(self.gt_dir, img_filename)).convert(\"RGB\")\n",
        "            \n",
        "        # Apply Data Augmentation\n",
        "        if self.transform is not None:\n",
        "            # Use set seed to make sure both LR and GT images get same transforms\n",
        "            seed = random.randint(0, 1e7)\n",
        "            torch.manual_seed(seed)\n",
        "            img_lr = self.transform(img_lr)\n",
        "            torch.manual_seed(seed)\n",
        "            img_gt = self.transform(img_gt)\n",
        "        else:\n",
        "            img_lr = torchvision.transforms.ToTensor()(img_lr)\n",
        "            img_gt = torchvision.transforms.ToTensor()(img_gt)\n",
        "            \n",
        "        # Apply Normalization from [0, 1] -> [-1, 1]\n",
        "        img_lr = (img_lr * 2) - 1.0\n",
        "        img_gt = (img_gt * 2) - 1.0\n",
        "\n",
        "        return {\n",
        "            'img_filename': img_filename,\n",
        "            'img_lr': img_lr,\n",
        "            'img_gt': img_gt\n",
        "        }\n",
        "\n",
        "\n",
        "# TODO: Change to match above\n",
        "class LowResDataSet(Dataset):\n",
        "    def __init__(self, lr_dir, memcache=False):\n",
        "        super().__init__()\n",
        "        self.lr_dir = lr_dir\n",
        "        self.memcache = memcache\n",
        "\n",
        "        # Attempt filename matching.\n",
        "        self.lr_image_filepaths = [f for f in lr_dir.glob('*') if is_image(f)]\n",
        "        self.image_filenames = sorted(list(map(os.path.basename, self.lr_image_filepaths)))\n",
        "        self.image_lr = []\n",
        "\n",
        "        # Load the images if we want to cache them in memory.\n",
        "        if self.memcache:\n",
        "            for i, img_filename in enumerate(self.image_filenames):\n",
        "                img_lr = np.array(Image.open(os.path.join(self.lr_dir, img_filename)).convert(\"RGB\")).astype(np.uint8)\n",
        "                img_lr = (img_lr / 127.5) - 1.0\n",
        "                img_lr = img_lr.transpose(2, 0, 1).astype(np.float32)\n",
        "                self.image_lr[i] = img_lr\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img_filename = self.image_filenames[i]\n",
        "        if self.memcache:\n",
        "            img_lr = self.image_lr[i]\n",
        "        else:\n",
        "            img_lr = np.array(Image.open(os.path.join(self.lr_dir, img_filename)).convert(\"RGB\")).astype(np.uint8)\n",
        "            img_lr = (img_lr / 127.5) - 1.0\n",
        "            img_lr = img_lr.transpose(2, 0, 1).astype(np.float32)\n",
        "        return {\n",
        "            'img_filename': img_filename,\n",
        "            'img_lr': img_lr\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSP4boF4N0Uq"
      },
      "source": [
        "### Metrics and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEyjTTUV7f5q",
        "outputId": "a2fa8633-a695-4721-ea6d-7144ec36e12d"
      },
      "source": [
        "memcache=True\n",
        "batch_size=8\n",
        "num_workers=multiprocessing.cpu_count()\n",
        "\n",
        "scale=4\n",
        "patch_size=24\n",
        "model_res_count=16\n",
        "\n",
        "transfer_generator_path=arg_util.path_abs(\"pretrained/SRResNet.pt\")\n",
        "\n",
        "# feat_layer='relu2_2'\n",
        "feat_layer='relu5_4'\n",
        "vgg_rescale_coeff=0.006\n",
        "adv_coeff=1e-3\n",
        "tv_loss_coeff=0.0\n",
        "\n",
        "t_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "t_device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g_v803wBgvf"
      },
      "source": [
        "import csv\n",
        "from skimage.color import rgb2ycbcr\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class MetricEval(object):\n",
        "    \"\"\"Object for evaluating metrics on each \"\"\"\n",
        "    def __init__(self, train_dataset, memcache=True, transform=None):\n",
        "        # Train, Validation, Test is the order of each list\n",
        "        self.modes = [\"train\", \"val\", \"test\"]\n",
        "        self.output_paths = [arg_util.path_abs(f\"{mode}_out/\") for mode in self.modes]\n",
        "        self.lr_paths = [arg_util.path_abs(f\"data/pokemon/lr/{mode}\") for mode in self.modes]\n",
        "        self.gt_paths = [arg_util.path_abs(f\"data/pokemon/hr/{mode}\") for mode in self.modes]\n",
        "\n",
        "        dataset_params = {\"memcache\": memcache, \"transform\": transform}\n",
        "        self.datasets = [train_dataset] + [\n",
        "            LowResGroundTruthDataset(lr_dir=lr_path, gt_dir=gt_path, **dataset_params)\n",
        "            for lr_path, gt_path in zip(self.lr_paths[1:], self.gt_paths[1:])\n",
        "        ]\n",
        "\n",
        "        loader_params = {\"batch_size\": batch_size, \"shuffle\": False, \"drop_last\": False, \"num_workers\": num_workers}\n",
        "        self.loaders = [DataLoader(dataset, **loader_params) for dataset in self.datasets]\n",
        "        \n",
        "        # self.L2_MSE_loss = nn.MSELoss()\n",
        "        # self.cross_ent = nn.BCELoss()\n",
        "        self.tv_loss = TVLoss()\n",
        "\n",
        "        self.vgg_net = None\n",
        "        self.vgg_loss = None\n",
        "        \n",
        "    def load_generator(self, generator_path=None, generator=None):\n",
        "        if generator_path is None and generator is None:\n",
        "            raise ValueError(f\"One of generator_path or generator must not be None.\")\n",
        "        \n",
        "        if generator_path:\n",
        "            self.generator = Generator(img_feat=3, n_feats=64, kernel_size=3, num_block=model_res_count, scale=scale)\n",
        "            self.generator.load_state_dict(torch.load(generator_path, map_location=t_device))\n",
        "            self.generator = self.generator.to(t_device)\n",
        "        else:\n",
        "            self.generator = generator\n",
        "        self.generator.eval()\n",
        "    \n",
        "    def get_metric(self, mode=\"val\", metric=\"MSE\", write_img=False):\n",
        "        # Valid Metrics: MSE, PSNR, VGG22, VGG54 (TODO: SSIM)\n",
        "        idx = self.modes.index(mode)\n",
        "        with torch.no_grad():\n",
        "            results = []\n",
        "            for lr_gt_datum in self.loaders[idx]:\n",
        "                img_filenames = lr_gt_datum['img_filename']\n",
        "                img_lrs = lr_gt_datum['img_lr'].to(t_device)\n",
        "                img_gts = lr_gt_datum['img_gt'].to(t_device)\n",
        "                \n",
        "                img_preds, _ = generator(img_lrs)\n",
        "\n",
        "                img_lrs.cpu()\n",
        "                img_gts.cpu()\n",
        "                img_preds.cpu()\n",
        "\n",
        "                # Revert from [-1, 1] -> [0, 1]\n",
        "                img_gts = ((img_gts + 1.) / 2.)\n",
        "                img_preds = ((torch.clip(img_preds, -1., 1.) + 1.) / 2.)\n",
        "                \n",
        "                # Resize GT to ensure its the same size as HR.\n",
        "                img_gts = img_gts[:, :, :img_preds.shape[2], :img_preds.shape[3]]\n",
        "                if metric == \"MSE\":\n",
        "                    loss = F.mse_loss(img_preds, img_gts)\n",
        "                    results.append(loss)\n",
        "                elif metric == \"PSNR\":\n",
        "                    # Calculate psnr from ycbcr comparison. (N, H, W, C)\n",
        "                    y_preds = img_preds.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "                    y_gt = img_gts.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "                    y_preds = rgb2ycbcr(y_preds)[:, scale:-scale, scale:-scale, 0]\n",
        "                    y_gt = rgb2ycbcr(y_gt)[:, scale:-scale, scale:-scale, 0]\n",
        "\n",
        "                    psnr = peak_signal_noise_ratio(y_gt / 255., y_preds / 255., data_range=1.)                        \n",
        "                    results.append(psnr)\n",
        "\n",
        "                elif metric == \"VGG22\" or metric == \"VGG54\":\n",
        "                    if self.vgg_net is None:\n",
        "                        self.vgg_net = vgg19().to(t_device)\n",
        "                        self.vgg_net = self.vgg_net.eval()\n",
        "                        self.vgg_loss = perceptual_loss(self.vgg_net)\n",
        "                    \n",
        "                    img_gts = img_gts.to(t_device)\n",
        "                    img_preds = img_preds.to(t_device)\n",
        "                    \n",
        "                    feat_layer = \"relu2_2\" if metric == \"VGG22\" else \"relu5_4\"\n",
        "                    _percep_loss, hr_feat, sr_feat = self.vgg_loss(img_gts, img_preds, layer=feat_layer)\n",
        "        \n",
        "                    L2_loss = F.mse_loss(img_preds, img_gts)\n",
        "                    percep_loss = vgg_rescale_coeff * _percep_loss\n",
        "                    total_variance_loss = tv_loss_coeff * self.tv_loss(vgg_rescale_coeff * (hr_feat - sr_feat)**2)\n",
        "\n",
        "                    g_loss = percep_loss + total_variance_loss + L2_loss\n",
        "                    results.append(g_loss)\n",
        "                    \n",
        "                    img_gts = img_gts.cpu()\n",
        "                    img_preds = img_preds.cpu()\n",
        "\n",
        "\n",
        "                if write_img:\n",
        "                    for i in range(len(img_filenames)):\n",
        "                        result = Image.fromarray((img_preds[i] * 255.).permute((1, 2, 0)).to(torch.uint8).cpu().numpy())\n",
        "                        result.save(self.output_paths[idx] / f\"pred_{img_filenames[i]}\")\n",
        "                        logging.info(f\"Inference Output: {self.output_paths[idx] / f'pred_{img_filenames[i]}'}\")\n",
        "                    \n",
        "            print(f\"Average {metric} Score: {sum(results)/len(results)}\")\n",
        "        return sum(results)/len(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ihS2t8cT7VM",
        "outputId": "42e03f48-dc92-40ab-8e3f-1e1af3013000"
      },
      "source": [
        "# Load Training Data\n",
        "generator_path_out = arg_util.path_abs(\"train_out/SRGAN_g.pt\")\n",
        "generator_path_out.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "discriminator_path_out = arg_util.path_abs(\"train_out/SRGAN_d.pt\")\n",
        "discriminator_path_out.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "checkpoint_dir = arg_util.path_abs(\"train_out/\")\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "gt_path = arg_util.path_abs(\"data/pokemon/hr/train/\")\n",
        "lr_path = arg_util.path_abs(\"data/pokemon/lr/train/\")\n",
        "\n",
        "lr_gt_dataset = LowResGroundTruthDataset(\n",
        "    lr_dir=lr_path, gt_dir=gt_path, memcache=memcache,\n",
        "    transform=aug\n",
        ")\n",
        "\n",
        "loader = DataLoader(lr_gt_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "generator = Generator(img_feat=3, n_feats=64, kernel_size=3, num_block=model_res_count, scale=scale)\n",
        "if transfer_generator_path:\n",
        "    generator.load_state_dict(torch.load(transfer_generator_path, map_location=t_device))\n",
        "    logging.info(f\"Loaded pre-trained model: {transfer_generator_path}\")\n",
        "    print(f\"Loaded pre-trained model: {transfer_generator_path}\")\n",
        "generator = generator.to(t_device)\n",
        "_ = generator.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pre-trained model: /content/drive/My Drive/CSC420/CSC420_project-main/src/pretrained/SRResNet.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgw5oobhDN4u"
      },
      "source": [
        "generator = Generator(img_feat=3, n_feats=64, kernel_size=3, num_block=model_res_count, scale=scale)\n",
        "generator.load_state_dict(torch.load(arg_util.path_abs(\"pretrained/SRGAN.pt\"), map_location=t_device))\n",
        "generator = generator.to(t_device)\n",
        "_ = generator.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY6sDgLAzUT-"
      },
      "source": [
        "metrics = MetricEval(lr_gt_dataset)\n",
        "metrics.load_generator(generator=generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54NSH5Duz6iR"
      },
      "source": [
        "# metrics.get_metric(mode=\"val\", metric=\"MSE\")\n",
        "# metrics.get_metric(mode=\"val\", metric=\"PSNR\")\n",
        "# metrics.get_metric(mode=\"val\", metric=\"VGG22\")\n",
        "# metrics.get_metric(mode=\"val\", metric=\"VGG54\")\n",
        "# Average MSE Score: 0.013258594088256359\n",
        "# Average PSNR Score: 20.03266583430994\n",
        "# Average VGG22 Score: 0.03304675221443176\n",
        "# Average VGG54 Score: 0.01411474496126175"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_ym_srvNkbF"
      },
      "source": [
        "## BELOW IS WORK IN PROGRESS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_j_7VRLOIcx"
      },
      "source": [
        "# metrics.get_metric(mode=\"val\", metric=\"VGG54\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGEZKdOrNPjV"
      },
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# print(torch.cuda.memory_summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjCshwaeAMXL"
      },
      "source": [
        "# Freeze all layer weights except the last few, needs testing\n",
        "for param in generator.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "for param in generator.last_conv.body.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# for param in generator.tail.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# for param in generator.conv02.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# for param in generator.body[15].parameters():\n",
        "#     param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Ij7hdqddmJ",
        "outputId": "3b44370d-c7e9-4886-e29c-673ce7bdb2a2"
      },
      "source": [
        "from pynvml import *\n",
        "nvmlInit()\n",
        "h = nvmlDeviceGetHandleByIndex(0)\n",
        "info = nvmlDeviceGetMemoryInfo(h)\n",
        "print(f'{info.total/(1024**2)} {info.free/(1024**2)} {info.used/(1024**2)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15079.75 411.875 14667.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT62cCG1NKoB"
      },
      "source": [
        "from arch.blocks import *\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \n",
        "    def __init__(self, img_feat = 3, n_feats = 64, kernel_size = 3, act = nn.LeakyReLU(inplace = True), num_of_block = 3, patch_size = 96):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.act = act\n",
        "        \n",
        "        self.conv01 = conv(in_channel = img_feat, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act)\n",
        "        self.conv02 = conv(in_channel = n_feats, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act, stride = 2)\n",
        "        \n",
        "        body = [discrim_block(in_feats = n_feats * (2 ** i), out_feats = n_feats * (2 ** (i + 1)), kernel_size = 3, act = self.act) for i in range(num_of_block)]    \n",
        "        self.body = nn.Sequential(*body)\n",
        "        \n",
        "        self.linear_size = 460800 # ((patch_size // (2 ** (num_of_block + 1))) ** 2) * (n_feats * (2 ** num_of_block))\n",
        "        \n",
        "        tail = []\n",
        "        \n",
        "        tail.append(nn.Linear(self.linear_size, 1024))\n",
        "        tail.append(self.act)\n",
        "        tail.append(nn.Linear(1024, 1))\n",
        "        tail.append(nn.Sigmoid())\n",
        "        \n",
        "        self.tail = nn.Sequential(*tail)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv01(x)\n",
        "        x = self.conv02(x)\n",
        "        x = self.body(x)\n",
        "        x = x.view(-1, self.linear_size)\n",
        "        x = self.tail(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzXDdqMXnWlZ"
      },
      "source": [
        "def train(init_lr=1e-4, pre_train_epoch=100, feat_layer=\"relu5_4\"):\n",
        "    # Initialize Losses\n",
        "    vgg_net = vgg19().to(t_device)\n",
        "    vgg_net = vgg_net.eval()\n",
        "    vgg_loss = perceptual_loss(vgg_net)\n",
        "    L2_MSE_loss = nn.MSELoss()\n",
        "    cross_ent = nn.BCELoss()\n",
        "    tv_loss = TVLoss()\n",
        "\n",
        "    real_label = torch.ones((batch_size, 1)).to(t_device)\n",
        "    fake_label = torch.zeros((batch_size, 1)).to(t_device)\n",
        "\n",
        "    g_optim = optim.Adam(generator.parameters(), lr=init_lr)\n",
        "    g_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(g_optim, mode=\"min\", factor=0.5, patience=30, cooldown=10, verbose=True)\n",
        "    global metrics\n",
        "\n",
        "    discriminator = Discriminator(patch_size = patch_size * scale)\n",
        "    discriminator = discriminator.to(t_device)\n",
        "    discriminator.train()\n",
        "\n",
        "    d_optim = optim.Adam(discriminator.parameters(), lr = 1e-4)\n",
        "    d_scheduler = optim.lr_scheduler.StepLR(g_optim, step_size = 200, gamma = 0.1)\n",
        "\n",
        "    checkpoint_modulo = (pre_train_epoch // 3) or pre_train_epoch\n",
        "    for pre_epoch in range(1, pre_train_epoch + 1):\n",
        "        logging.info(f\"Pre-train Epoch [{pre_epoch}]: running.\")\n",
        "\n",
        "        for _ in range(5):\n",
        "            for batch_i, lr_gt_datum in enumerate(loader):\n",
        "                ## Training Discriminator\n",
        "                img_lr, img_gt = lr_gt_datum['img_lr'].to(t_device), lr_gt_datum['img_gt'].to(t_device)\n",
        "                img_pred, _ = generator(img_lr)\n",
        "\n",
        "                img_gt = img_gt[:, :, :img_pred.shape[2], :img_pred.shape[3]]\n",
        "\n",
        "                fake_prob = discriminator(img_pred)\n",
        "                real_prob = discriminator(img_gt)\n",
        "                print(img_gt.size(), img_pred.size(), fake_prob.size(), real_prob.size())\n",
        "                \n",
        "                d_loss_real = cross_ent(real_prob, real_label)\n",
        "                d_loss_fake = cross_ent(fake_prob, fake_label)\n",
        "                \n",
        "                d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "                g_optim.zero_grad()\n",
        "                d_optim.zero_grad()\n",
        "                d_loss.backward()\n",
        "                d_optim.step()\n",
        "\n",
        "                print(\"Generator Loss:\", d_loss.item())\n",
        "                        \n",
        "            d_scheduler.step()\n",
        "\n",
        "        results = []\n",
        "        for batch_i, lr_gt_datum in enumerate(loader):\n",
        "            img_lr, img_gt = lr_gt_datum['img_lr'].to(t_device), lr_gt_datum['img_gt'].to(t_device)\n",
        "            img_pred, _ = generator(img_lr)\n",
        "\n",
        "            img_gt = ((img_gt + 1.) / 2.)\n",
        "            img_pred = ((torch.clip(img_pred, -1., 1.) + 1.) / 2.)\n",
        "            \n",
        "            # Resize GT to ensure its the same size as HR.\n",
        "            img_gt = img_gt[:, :, :img_pred.shape[2], :img_pred.shape[3]]\n",
        "\n",
        "            ## Training Generator\n",
        "            fake_prob = discriminator(img_pred)\n",
        "            _percep_loss, hr_feat, sr_feat = vgg_loss(img_gt, img_pred, layer=feat_layer)\n",
        "\n",
        "            g_loss = L2_MSE_loss(img_pred, img_gt) + \\\n",
        "                vgg_rescale_coeff * _percep_loss + \\\n",
        "                adv_coeff * cross_ent(fake_prob, real_label) + \\\n",
        "                tv_loss_coeff * tv_loss(vgg_rescale_coeff * (hr_feat - sr_feat)**2)\n",
        "            print(g_loss.item(), end=\" \")\n",
        "\n",
        "            d_optim.zero_grad()\n",
        "            g_optim.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optim.step()\n",
        "\n",
        "            results.append(g_loss.item())\n",
        "\n",
        "        # Log epoch statistics.\n",
        "        logging.info(f\"Pre-train Epoch [{pre_epoch}]: Average Train loss={sum(results)/len(results)}\")\n",
        "        print(f\"Pre-train Epoch [{pre_epoch}]: Average Train loss={sum(results)/len(results)}\")\n",
        "        with open(f\"results_{init_lr}\", \"a\") as fp:\n",
        "            fp.write(f\"Pre-train Epoch [{pre_epoch}]: Average Train loss={sum(results)/len(results)}\")\n",
        "\n",
        "        metrics.load_generator(generator=generator)\n",
        "        psnr = metrics.get_metric(mode=\"val\", metric=\"PSNR\")\n",
        "        vgg22 = metrics.get_metric(mode=\"val\", metric=\"VGG22\")\n",
        "        vgg54 = metrics.get_metric(mode=\"val\", metric=\"VGG54\")\n",
        "        \n",
        "        generator.train()\n",
        "        \n",
        "        g_scheduler.step(vgg54)\n",
        "\n",
        "        if pre_epoch % checkpoint_modulo == 0:\n",
        "            checkpoint_filepath = (checkpoint_dir / f'pre_trained_model_{pre_epoch}.pt').absolute()\n",
        "            torch.save(generator.state_dict(),  checkpoint_filepath)\n",
        "            logging.info(f\"Pre-train Epoch [{pre_epoch}]: saved model checkpoint: {checkpoint_filepath}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "hA2xnG1pZhO7",
        "outputId": "66e88452-f2d6-46e0-dc21-4e96aaf75351"
      },
      "source": [
        "# Average MSE Score: 0.013258594088256359\n",
        "# Average PSNR Score: 20.03266583430994\n",
        "# Average VGG22 Score: 0.03304675221443176\n",
        "# Average VGG54 Score: 0.01411474496126175\n",
        "# generator = Generator(img_feat=3, n_feats=64, kernel_size=3, num_block=model_res_count, scale=scale)\n",
        "# if transfer_generator_path:\n",
        "#     generator.load_state_dict(torch.load(transfer_generator_path, map_location=t_device))\n",
        "#     print(f\"Loaded pre-trained model: {transfer_generator_path}\")\n",
        "# generator = generator.to(t_device)\n",
        "generator.train()\n",
        "\n",
        "train(init_lr=1e-9, pre_train_epoch=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-9792f9b20f6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_train_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-90ce8eb773ef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(init_lr, pre_train_epoch, feat_layer)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 14.73 GiB total capacity; 12.27 GiB already allocated; 419.88 MiB free; 13.38 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khpMJqHQxMms"
      },
      "source": [
        "for lr in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]:\n",
        "    generator = Generator(img_feat=3, n_feats=64, kernel_size=3, num_block=model_res_count, scale=scale)\n",
        "    if transfer_generator_path:\n",
        "        generator.load_state_dict(torch.load(transfer_generator_path, map_location=t_device))\n",
        "        print(f\"Loaded pre-trained model: {transfer_generator_path}\")\n",
        "    generator = generator.to(t_device)\n",
        "    _ = generator.train()\n",
        "\n",
        "    train(init_lr=lr, pre_train_epoch=1000)\n",
        "    generator_path_out = arg_util.path_abs(f\"train_out/vgg54_{lr}.pt\")\n",
        "    generator_path_out.parent.mkdir(parents=True, exist_ok=True)\n",
        "    torch.save(generator.state_dict(), generator_path_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzQ2SwhkfJK1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L6bzUKsK22h"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s20UlfpEATgf"
      },
      "source": [
        "generator_path_out = arg_util.path_abs(\"train_out/9_SRResnet_pre.pt\")\n",
        "generator_path_out.parent.mkdir(parents=True, exist_ok=True)\n",
        "torch.save(generator.state_dict(), generator_path_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2P4RRzAtQ1A"
      },
      "source": [
        "metrics.get_metric(mode=\"test\", metric=\"VGG54\", write_img=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzv85G0jXLOh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "        \n",
        "# Show Results\n",
        "pred_fp = \"test_out/\"\n",
        "real_fp = \"data/pokemon/hr/test/\"\n",
        "bad_fp = \"data/pokemon/lr/test/\"\n",
        "\n",
        "# generator.eval()\n",
        "with torch.no_grad():\n",
        "    for filename in os.listdir(real_fp):\n",
        "        if np.random.random() < 0.7:\n",
        "            continue\n",
        "\n",
        "        print(filename)\n",
        "        pred = cv2.imread(f'{pred_fp}pred_{filename}')\n",
        "        real = cv2.imread(f'{real_fp}{filename}')\n",
        "        lr = cv2.imread(f'{bad_fp}{filename}')\n",
        "\n",
        "        f, ax = plt.subplots(1, 3, figsize=(8., 8.))\n",
        "        ax[0].imshow(real)\n",
        "        ax[0].axis('off')\n",
        "        ax[1].imshow(pred)\n",
        "        ax[1].axis('off')\n",
        "        ax[2].imshow(lr)\n",
        "        ax[2].axis('off')\n",
        "\n",
        "        plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
        "        plt.show()\n",
        "        # break\n",
        "\n",
        "# _ = generator.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGJMwgSTwRva"
      },
      "source": [
        "vgg_net = vgg19().to(device)\n",
        "vgg_net = vgg_net.eval()\n",
        "\n",
        "discriminator = Discriminator(patch_size = args.patch_size * args.scale)\n",
        "discriminator = discriminator.to(device)\n",
        "discriminator.train()\n",
        "\n",
        "d_optim = optim.Adam(discriminator.parameters(), lr = 1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(g_optim, step_size = 2000, gamma = 0.1)\n",
        "\n",
        "VGG_loss = perceptual_loss(vgg_net)\n",
        "cross_ent = nn.BCELoss()\n",
        "tv_loss = TVLoss()\n",
        "real_label = torch.ones((args.batch_size, 1)).to(device)\n",
        "fake_label = torch.zeros((args.batch_size, 1)).to(device)\n",
        "\n",
        "while fine_epoch < args.fine_train_epoch:\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    for i, tr_data in enumerate(loader):\n",
        "        gt = tr_data['GT'].to(device)\n",
        "        lr = tr_data['LR'].to(device)\n",
        "                    \n",
        "        ## Training Discriminator\n",
        "        output, _ = generator(lr)\n",
        "        fake_prob = discriminator(output)\n",
        "        real_prob = discriminator(gt)\n",
        "        \n",
        "        d_loss_real = cross_ent(real_prob, real_label)\n",
        "        d_loss_fake = cross_ent(fake_prob, fake_label)\n",
        "        \n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        g_optim.zero_grad()\n",
        "        d_optim.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optim.step()\n",
        "        \n",
        "        ## Training Generator\n",
        "        output, _ = generator(lr)\n",
        "        fake_prob = discriminator(output)\n",
        "        \n",
        "        _percep_loss, hr_feat, sr_feat = VGG_loss((gt + 1.0) / 2.0, (output + 1.0) / 2.0, layer = args.feat_layer)\n",
        "        \n",
        "        L2_loss = l2_loss(output, gt)\n",
        "        percep_loss = args.vgg_rescale_coeff * _percep_loss\n",
        "        adversarial_loss = args.adv_coeff * cross_ent(fake_prob, real_label)\n",
        "        total_variance_loss = args.tv_loss_coeff * tv_loss(args.vgg_rescale_coeff * (hr_feat - sr_feat)**2)\n",
        "        \n",
        "        g_loss = percep_loss + adversarial_loss + total_variance_loss + L2_loss\n",
        "        \n",
        "        g_optim.zero_grad()\n",
        "        d_optim.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optim.step()\n",
        "\n",
        "        \n",
        "    fine_epoch += 1\n",
        "\n",
        "    if fine_epoch % 2 == 0:\n",
        "        print(fine_epoch)\n",
        "        print(g_loss.item())\n",
        "        print(d_loss.item())\n",
        "        print('=========')\n",
        "\n",
        "    if fine_epoch % 500 ==0:\n",
        "        torch.save(generator.state_dict(), './model/SRGAN_gene_%03d.pt'%fine_epoch)\n",
        "        torch.save(discriminator.state_dict(), './model/SRGAN_discrim_%03d.pt'%fine_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWmYkFkZXp0v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k92H3Di-uRg9"
      },
      "source": [
        "# Below is Work In Progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy-gBd561z9p"
      },
      "source": [
        "# Train using perceptual & adversarial loss\n",
        "if adversarial_train_epoch > 0:\n",
        "    logging.info(f\"Training using Adversarial loss for {adversarial_train_epoch} epochs.\")\n",
        "\n",
        "    # Set-up adversarial loss VGG network.\n",
        "    vgg_net = vgg19().to(t_device)\n",
        "    vgg_net = vgg_net.eval()\n",
        "\n",
        "    discriminator = Discriminator(patch_size=patch_size * scale)\n",
        "    discriminator = discriminator.to(t_device)\n",
        "    discriminator.train()\n",
        "\n",
        "    d_optim = optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(g_optim, step_size=2000, gamma=0.1)\n",
        "\n",
        "    VGG_loss = perceptual_loss(vgg_net)\n",
        "    cross_ent = nn.BCELoss()\n",
        "    tv_loss = TVLoss()\n",
        "    base_real_label = torch.ones((batch_size, 1)).to(t_device)\n",
        "    base_fake_label = torch.zeros((batch_size, 1)).to(t_device)\n",
        "\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    checkpoint_modulo = (adversarial_train_epoch // 10) or adversarial_train_epoch\n",
        "    for epoch in range(1, adversarial_train_epoch + 1):\n",
        "        logging.info(f\"Epoch [{epoch}]: running.\")\n",
        "\n",
        "        d_optim.step()\n",
        "        g_optim.step()\n",
        "        scheduler.step()\n",
        "        for batch_i, lr_gt_datum in enumerate(loader):\n",
        "            img_lr, img_gt = lr_gt_datum['img_lr'].to(t_device), lr_gt_datum['img_gt'].to(t_device)\n",
        "            img_hr_prediction, _ = generator(img_lr)\n",
        "\n",
        "            # Train Discriminator\n",
        "            fake_prob = discriminator(img_hr_prediction)\n",
        "            real_prob = discriminator(img_gt)\n",
        "\n",
        "            # Avoid mismatched label and probability length in case where batch is remainder of data, but not\n",
        "            # a perfect fit.\n",
        "            real_label = base_real_label\n",
        "            fake_label = base_fake_label\n",
        "            if len(base_real_label) != len(real_prob):\n",
        "                real_label = torch.ones((len(real_prob), 1)).to(t_device)\n",
        "                fake_label = torch.zeros((len(real_prob), 1)).to(t_device)\n",
        "\n",
        "            d_loss_real = cross_ent(real_prob, real_label)\n",
        "            d_loss_fake = cross_ent(fake_prob, fake_label)\n",
        "\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "            # Back-propagate Discriminator\n",
        "            g_optim.zero_grad()\n",
        "            d_optim.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_optim.step()\n",
        "\n",
        "            # Train Generator\n",
        "            img_hr_prediction, _ = generator(img_lr)\n",
        "            fake_prob = discriminator(img_hr_prediction)\n",
        "\n",
        "            l2_loss = L2_MSE_loss(img_hr_prediction, img_gt)\n",
        "            percep_loss, hr_feat, sr_feat = VGG_loss((img_gt + 1.0) / 2.0, (img_hr_prediction + 1.0) / 2.0, layer=feat_layer)\n",
        "            percep_loss = vgg_rescale_coeff * percep_loss\n",
        "            adversarial_loss = adv_coeff * cross_ent(fake_prob, real_label)\n",
        "            total_variance_loss = tv_loss_coeff * tv_loss(vgg_rescale_coeff * (hr_feat - sr_feat) ** 2)\n",
        "            g_loss = percep_loss + adversarial_loss + total_variance_loss + l2_loss\n",
        "\n",
        "            # Back-propagate Generator\n",
        "            g_optim.zero_grad()\n",
        "            d_optim.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optim.step()\n",
        "\n",
        "        # Log epoch statistics.\n",
        "        logging.info(f\"Epoch [{epoch}]: g_loss={g_loss.item()} d_loss={d_loss.item()}\")\n",
        "        if epoch % checkpoint_modulo == 0:\n",
        "            g_checkpoint_filepath = (checkpoint_dir / f'SRGAN_g_{epoch}.pt').absolute()\n",
        "            d_checkpoint_filepath = (checkpoint_dir / f'SRGAN_d_{epoch}.pt').absolute()\n",
        "            torch.save(generator.state_dict(),  g_checkpoint_filepath)\n",
        "            torch.save(discriminator.state_dict(), d_checkpoint_filepath)\n",
        "            logging.info(f\"Pre-train Epoch [{epoch}]: saved model checkpoints: {g_checkpoint_filepath}, {d_checkpoint_filepath}\")\n",
        "    if discriminator_path_out:\n",
        "        torch.save(discriminator.state_dict(), discriminator_path_out)\n",
        "torch.save(generator.state_dict(), generator_path_out)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}