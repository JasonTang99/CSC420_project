@misc{Thomas2019,
	title        = {{Deep learning based super resolution, without using a GAN}},
	author       = {Thomas, Christopher},
	year         = 2019,
	booktitle    = {Towards Data Science},
	url          = {https://towardsdatascience.com/deep-learning-based-super-resolution-without-using-a-gan-11c9bb5b6cd5},
	urldate      = {2020-12-19},
	mendeley-groups = {CSC420Report}
}
@inproceedings{Bae2020,
	title        = {{Delta-STN: Efficient Bilevel Optimization for Neural Networks using Structured Response Jacobians}},
	author       = {Bae, Juhan and Grosse, Roger},
	year         = 2020,
	month        = oct,
	booktitle    = {34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
	publisher    = {Neural information processing systems foundation},
	address      = {Vancouver, Canada},
	url          = {http://arxiv.org/abs/2010.13514},
	abstract     = {Hyperparameter optimization of neural networks can be elegantly formulated as a bilevel optimization problem. While research on bilevel optimization of neural networks has been dominated by implicit differentiation and unrolling, hypernetworks such as Self-Tuning Networks (STNs) have recently gained traction due to their ability to amortize the optimization of the inner objective. In this paper, we diagnose several subtle pathologies in the training of STNs. Based on these observations, we propose the {\$}\backslashDelta{\$}-STN, an improved hypernetwork architecture which stabilizes training and optimizes hyperparameters much more efficiently than STNs. The key idea is to focus on accurately approximating the best-response Jacobian rather than the full best-response function; we achieve this by reparameterizing the hypernetwork and linearizing the network around the current parameters. We demonstrate empirically that our {\$}\backslashDelta{\$}-STN can tune regularization hyperparameters (e.g. weight decay, dropout, number of cutout holes) with higher accuracy, faster convergence, and improved stability compared to existing approaches.},
	archiveprefix = {arXiv},
	arxivid      = {2010.13514},
	eprint       = {2010.13514},
	file         = {:C\$\backslash\$:/Users/Adam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bae, Grosse - 2020 - Delta-STN Efficient Bilevel Optimization for Neural Networks using Structured Response Jacobians.pdf:pdf},
	mendeley-groups = {CSC420Report}
}
@article{Takano2019,
	title        = {{SRGAN: Training Dataset Matters}},
	author       = {Takano, Nao and Alaghband, Gita},
	year         = 2019,
	month        = mar,
	journal      = {arXiv},
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/1903.09922},
	abstract     = {Generative Adversarial Networks (GANs) in supervised settings can generate photo-realistic corresponding output from low-definition input (SRGAN). Using the architecture presented in the SRGAN original paper [2], we explore how selecting a dataset affects the outcome by using three different datasets to see that SRGAN fundamentally learns objects, with their shape, color, and texture, and redraws them in the output rather than merely attempting to sharpen edges. This is further underscored with our demonstration that once the network learns the images of the dataset, it can generate a photo-like image with even a slight hint of what it might look like for the original from a very blurry edged sketch. Given a set of inference images, the network trained with the same dataset results in a better outcome over the one trained with arbitrary set of images, and we report its significance numerically with Frechet Inception Distance score [22].},
	archiveprefix = {arXiv},
	arxivid      = {1903.09922},
	eprint       = {1903.09922},
	file         = {:C\$\backslash\$:/Users/Adam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Takano, Alaghband - 2019 - SRGAN Training Dataset Matters.pdf:pdf},
	keywords     = {FID,GAN,SRGAN},
	mendeley-groups = {CSC420Report}
}
@misc{Tsang2020,
	title        = {{Review: SRGAN {\&} SRResNet -- Photo-Realistic Super Resolution (GAN {\&} Super Resolution)}},
	author       = {Tsang, Sik-ho},
	year         = 2020,
	booktitle    = {Medium},
	url          = {https://sh-tsang.medium.com/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490},
	urldate      = {2020-12-19},
	mendeley-groups = {CSC420Report}
}
@misc{Tsang2018,
	title        = {{Review: ResNet -- Winner of ILSVRC 2015 (Image Classification, Localization, Detection)}},
	author       = {Tsang, Sik-ho},
	year         = 2018,
	booktitle    = {Towards Data Science},
	url          = {https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8},
	urldate      = {2020-12-19},
	mendeley-groups = {CSC420Report}
}
@misc{Hui2018,
	title        = {{GAN -- Super Resolution GAN (SRGAN)}},
	author       = {Hui, Jonathan},
	year         = 2018,
	booktitle    = {Medium},
	url          = {https://jonathan-hui.medium.com/gan-super-resolution-gan-srgan-b471da7270ec},
	urldate      = {2020-12-19},
	mendeley-groups = {CSC420Report}
}
@inproceedings{Ledig2017,
	title        = {{Photo-realistic single image super-resolution using a generative adversarial network}},
	author       = {Ledig, Christian and Theis, Lucas and Husz{\'{a}}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	year         = 2017,
	month        = nov,
	booktitle    = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	publisher    = {Institute of Electrical and Electronics Engineers Inc.},
	volume       = {2017-January},
	pages        = {105--114},
	doi          = {10.1109/CVPR.2017.19},
	isbn         = 9781538604571,
	url          = {https://arxiv.org/abs/1609.04802v5},
	abstract     = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image superresolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4\texttimes{} upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	archiveprefix = {arXiv},
	arxivid      = {1609.04802},
	eprint       = {1609.04802},
	file         = {:C\$\backslash\$:/Users/Adam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ledig et al. - 2017 - Photo-realistic single image super-resolution using a generative adversarial network.pdf:pdf},
	mendeley-groups = {CSC420Report}
}
@inproceedings{Lim2017,
	title        = {{Enhanced Deep Residual Networks for Single Image Super-Resolution}},
	author       = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
	year         = 2017,
	month        = aug,
	booktitle    = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	publisher    = {IEEE Computer Society},
	volume       = {2017-July},
	pages        = {1132--1140},
	doi          = {10.1109/CVPRW.2017.151},
	isbn         = 9781538607336,
	issn         = 21607516,
	url          = {https://arxiv.org/abs/1707.02921v1},
	abstract     = {Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge[26].},
	archiveprefix = {arXiv},
	arxivid      = {1707.02921},
	eprint       = {1707.02921},
	mendeley-groups = {CSC420Report}
}
@inproceedings{Park2018,
	title        = {{SRFeat: Single Image Super-Resolution with Feature Discrimination}},
	author       = {Park, Seong Jin and Son, Hyeongseok and Cho, Sunghyun and Hong, Ki Sang and Lee, Seungyong},
	year         = 2018,
	month        = sep,
	booktitle    = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	publisher    = {Springer Verlag},
	volume       = {11220 LNCS},
	pages        = {455--471},
	doi          = {10.1007/978-3-030-01270-0\_27},
	isbn         = 9783030012694,
	issn         = 16113349,
	url          = {https://link.springer.com/chapter/10.1007/978-3-030-01270-0{\%5F}27},
	abstract     = {Generative adversarial networks (GANs) have recently been adopted to single image super-resolution (SISR) and showed impressive results with realistically synthesized high-frequency textures. However, the results of such GAN-based approaches tend to include less meaningful high-frequency noise that is irrelevant to the input image. In this paper, we propose a novel GAN-based SISR method that overcomes the limitation and produces more realistic results by attaching an additional discriminator that works in the feature domain. Our additional discriminator encourages the generator to produce structural high-frequency features rather than noisy artifacts as it distinguishes synthetic and real images in terms of features. We also design a new generator that utilizes long-range skip connections so that information between distant layers can be transferred more effectively. Experiments show that our method achieves the state-of-the-art performance in terms of both PSNR and perceptual quality compared to recent GAN-based methods.},
	keywords     = {Adversarial network,High frequency features,Perceptual quality,Super-resolution},
	mendeley-groups = {CSC420Report}
}
@misc{Kanska2017,
	title        = {{Using deep learning for Single Image Super Resolution}},
	author       = {Ka{\'{n}}ska, Katarzyna},
	year         = 2017,
	booktitle    = {deepsense.ai},
	url          = {https://deepsense.ai/using-deep-learning-for-single-image-super-resolution/},
	urldate      = {2020-12-19},
	mendeley-groups = {CSC420Report}
}
@article{Yang2019,
	title        = {{Deep Learning for Single Image Super-Resolution: A Brief Review}},
	author       = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing Hao and Liao, Qingmin},
	year         = 2019,
	month        = dec,
	journal      = {IEEE Transactions on Multimedia},
	publisher    = {Institute of Electrical and Electronics Engineers Inc.},
	volume       = 21,
	number       = 12,
	pages        = {3106--3121},
	doi          = {10.1109/TMM.2019.2919431},
	issn         = 19410077,
	url          = {https://arxiv.org/abs/1808.03344v3},
	abstract     = {Single image super-resolution (SISR) is a notoriously challenging ill-posed problem that aims to obtain a high-resolution output from one of its low-resolution versions. Recently, powerful deep learning algorithms have been applied to SISR and have achieved state-of-the-art performance. In this survey, we review representative deep learning-based SISR methods and group them into two categories according to their contributions to two essential aspects of SISR: The exploration of efficient neural network architectures for SISR and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is first established, and several critical limitations of the baseline are summarized. Then, representative works on overcoming these limitations are presented based on their original content, as well as our critical exposition and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally, we conclude this review with some current challenges and future trends in SISR that leverage deep learning algorithms.},
	archiveprefix = {arXiv},
	arxivid      = {1808.03344},
	eprint       = {1808.03344},
	file         = {:C\$\backslash\$:/Users/Adam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2019 - Deep Learning for Single Image Super-Resolution A Brief Review.pdf:pdf},
	keywords     = {Single image super-resolution,deep learning,neural networks,objective function},
	mendeley-groups = {CSC420Report}
}
@article{Shorten2019,
	title        = {{A survey on Image Data Augmentation for Deep Learning}},
	author       = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year         = 2019,
	month        = dec,
	journal      = {Journal of Big Data},
	publisher    = {SpringerOpen},
	volume       = 6,
	number       = 1,
	pages        = 60,
	doi          = {10.1186/s40537-019-0197-0},
	issn         = 21961115,
	url          = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
	abstract     = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	file         = {:C\$\backslash\$:/Users/Adam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shorten, Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learning.pdf:pdf},
	keywords     = {Big data,Data Augmentation,Deep Learning,GANs,Image data},
	mendeley-groups = {CSC420Report}
}
@misc{SonGitHub,
author = {Son, Donghee and Park, Seobin},
booktitle = {GitHub},
mendeley-groups = {CSC420Report},
title = {{dongheehand/SRGAN-PyTorch: SRGAN (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network) implementation using PyTorch framework}},
url = {https://github.com/dongheehand/SRGAN-PyTorch},
urldate = {2020-12-19}
}
@InProceedings{10.1007/978-3-319-95957-3_50,
author="Xu, Fengchi
and Yan, Zifei
and Xiao, Gang
and Zhang, Kai
and Zuo, Wangmeng",
editor="Huang, De-Shuang
and Gromiha, M. Michael
and Han, Kyungsook
and Hussain, Abir",
title="JPEG Image Super-Resolution via Deep Residual Network",
booktitle="Intelligent Computing Methodologies",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="472--483",
abstract="In many practical scenarios, the images to be super-resolved are not only of low resolution (LR) but also JPEG compressed, while most of the existing super-resolution methods assume compression free LR image inputs. As a result, the JPEG compression artifacts (e.g., blocking artifacts) are often exacerbated in the super-resolved images, leading to unpleasant visual results. In this paper, we address this problem via learning a deep residual convolutional neural network (CNN) that exploits a skips-in-skip connection. More specifically, by increasing the network depth to 31 layers with receptive field of 63 by 63, we train a single CNN model which is able to handle JPEG image super-resolution with various combinations of scale and quality factors, as well as the extreme cases, i.e., image super-resolution with multiple scale factors, and JPEG image deblocking with different quality factors. Our extensive experimental results demonstrate that the proposed deep model can not only yield high resolution (HR) images that are visually more pleasant than those state-of-the-art deblocking and super-resolution methods in a cascaded manner, but also deliver very competitive results with the state-of-the-art super-resolution methods and JPEG deblocking methods in terms of quantitative and qualitative measures.",
isbn="978-3-319-95957-3"
}

@misc{frégier2020mind2mind,
      title={Mind2Mind : transfer learning for GANs}, 
      author={Yaël Frégier and Jean-Baptiste Gouray},
      year={2020},
      eprint={1906.11613},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}